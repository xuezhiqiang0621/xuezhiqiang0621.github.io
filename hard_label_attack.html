
<!doctype html>
<html lang="en">
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>BugTorch</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- New GA4 tracking code, see https://support.google.com/analytics/answer/10271001#analyticsjs-enable-basic --> 
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GNJD50R0Z7"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-GNJD50R0Z7');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
      .social-row {
        display: flex;
        flex-wrap: wrap;
        justify-content: space-between;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>BugTorch</h1>
        <p>Associate Professor of Economics<br>University of Oklahoma</p>
        <p>Research Affiliate<br><a href="http://legacy.iza.org/en/webcontent/personnel/photos/index_html?key=24155">Institute for the Study of Labor (IZA)</a></p>
        <p>Fellow<br><a href="https://glabor.org/user/tyleransom/">Global Labor Organization (GLO)</a></p>
    <h3><a href="https://www.bugtorch.org/">Home</a></h3>
	<h3><a href="https://xuezhiqiang0621.github.io/hard_label_attack.html">Hard_Lable_Attack</a></h3>

    <b>Social</b><br>
        <div class="social-row">
          <a href="mailto:ransom@ou.edu" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
          <a href="https://scholar.google.com/citations?user=eohlTTcAAAAJ&hl=en" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
          <a href="https://orcid.org/0000-0002-6910-0363"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
          <a href="http://ideas.repec.org/f/pra541.html"><i class="ai ai-fw ai-ideas-repec-square"></i> RePEc</a><br>
          <a href="http://github.com/tyleransom"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
          <a href="http://twitter.com/tyleransom" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
          <a href="http://linkedin.com/in/tyleransom" class="author-social" target="_blank"><i class="fa fa-fw fa-linkedin-square"></i> LinkedIn</a><br>
          <br>
        </div>
        <br>

    <p><b>Contact:</b><br>Department of Economics<br>University of Oklahoma<br>322 CCD1, 308 Cate Center Drive<br>Norman, OK 73072</p>
    <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>

      </header>
      <section>

    <!-- No R&Rs or WPs at the moment
    <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Papers under Revision to Resubmit</h2>
    <p style="margin:0"> Coming soon. <br> <br> 

    <hr>

    <h2><a id="recent-papers-updated" class="anchor" href="#workingpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Working Papers</h2>
    <p style="margin:0"> Coming soon. <br> <br> 

    <hr>                    
    -->

    <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Published &amp; Forthcoming Papers</h2>
    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/HopSkipJumpAttack_A_Query-Efficient_Decision-Based_Attack.pdf">HopSkipJumpAttack: A Query-Efficient Decision-Based Attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for \(l_2\) and \(l_\infty\) similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks. It also achieves competitive performance in attacking several widely-used defense mechanisms. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/hop_skip_jump_attack/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/hop_skip_jump_attack/attack.py">code下载</a> <br>  </p></div><br>
    
    





    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/Aha!%20Adaptive%20History-driven%20Attack%20for%20Decision-based%20Black-box%20Models.pdf">Aha! Adaptive History-driven Attack for Decision-based Black-box Models</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> The decision-based black-box attack means to craft adversarial examples with only the top-1 label of the victim
model available. A common practice is to start from a large
perturbation and then iteratively reduce it with a deterministic direction and a random one while keeping it adversarial. The limited information obtained from each query and
inefficient direction sampling impede attack efficiency, making it hard to obtain a small enough perturbation within a
limited number of queries. To tackle this problem, we propose a novel attack method termed Adaptive History-driven
Attack (AHA) which gathers information from all historical queries as the prior for current sampling. Moreover, to
balance between the deterministic direction and the random
one, we dynamically adjust the coefficient according to the
ratio of the actual magnitude reduction to the expected one.
Such a strategy improves the success rate of queries during optimization, letting adversarial examples move swiftly
along the decision boundary. Our method can also integrate
with subspace optimization like dimension reduction to further improve efficiency. Extensive experiments on both ImageNet and CelebA datasets demonstrate that our method
achieves at least 24.3% lower magnitude of perturbation
on average with the same number of queries. Finally, we
prove the practical potential of our method by evaluating it
on popular defense methods and a real-world system provided by MEGVII Face++. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/AHA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/AHA/AHA.zip">code下载</a> <br>  </p></div><br>






    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/BO-DBA%EF%BC%9A%20Query-Efficient%20Decision-Based%20Adversarial%20Attacks%20via%20Bayesian%20Optimization.pdf">BO-DBA： Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Decision-based attacks (DBA), wherein attackers perturb inputs to spoof learning algorithms by
observing solely the output labels, are a type of severe adversarial attacks against Deep Neural
Networks (DNNs) requiring minimal knowledge of attackers. State-of-the-art DBA attacks relying
on zeroth-order gradient estimation require an excessive number of queries. Recently, Bayesian
optimization (BO) has shown promising in reducing the number of queries in score-based attacks
(SBA), in which attackers need to observe real-valued probability scores as outputs. However,
extending BO to the setting of DBA is nontrivial because in DBA only output labels instead of
real-valued scores, as needed by BO, are available to attackers. In this paper, we close this gap by
proposing an efficient DBA attack, namely BO-DBA. Different from existing approaches, BO-DBA
generates adversarial examples by searching so-called directions of perturbations. It then formulates
the problem as a BO problem that minimizes the real-valued distortion of perturbations. With the
optimized perturbation generation process, BO-DBA converges much faster than the state-of-the-art
DBA techniques. Experimental results on pre-trained ImageNet classifiers show that BO-DBA
converges within 200 queries while the state-of-the-art DBA techniques need over 15,000 queries to
achieve the same level of perturbation distortion. BO-DBA also shows similar attack success rates
even as compared to BO-based SBA attacks but with less distortion. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/BO_DBA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/BO_DBA/BO_DBA.zip">code下载</a> <br>  </p></div><br>




    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/CGBA%EF%BC%9A%20Curvature-aware%20Geometric%20Black-box%20Attack.pdf">CGBA： Curvature-aware Geometric Black-box Attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Decision-based black-box attacks often necessitate a
large number of queries to craft an adversarial example. Moreover, decision-based attacks based on querying
boundary points in the estimated normal vector direction
often suffer from inefficiency and convergence issues. In
this paper, we propose a novel query-efficient c
¯
urvatureaware g
¯
eometric decision-based b
¯
lack-box a
¯
ttack (CGBA)
that conducts boundary search along a semicircular path
on a restricted 2D plane to ensure finding a boundary point
successfully irrespective of the boundary curvature. While
the proposed CGBA attack can work effectively for an arbitrary decision boundary, it is particularly efficient in exploiting the low curvature to craft high-quality adversarial examples, which is widely seen and experimentally verified in commonly used classifiers under non-targeted attacks. In contrast, the decision boundaries often exhibit
higher curvature under targeted attacks. Thus, we develop
a new query-efficient variant, CGBA-H, that is adapted for
the targeted attack. In addition, we further design an algorithm to obtain a better initial boundary point at the expense
of some extra queries, which considerably enhances the
performance of the targeted attack. Extensive experiments
are conducted to evaluate the performance of our proposed
methods against some well-known classifiers on the ImageNet and CIFAR10 datasets, demonstrating the superiority
of CGBA and CGBA-H over state-of-the-art non-targeted
and targeted attacks, respectively. The source code is available at https://github.com/Farhamdur/CGBA. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/CGBA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/CGBA/CGBA.zip">code下载</a> <br>  </p></div><br>











    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/GeoDA%EF%BC%9A%20a%20geometric%20framework%20for%20black-box%20adversarial%20attacks.pdf">GeoDA： a geometric framework for black-box adversarial attacks</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Adversarial examples are known as carefully perturbed
images fooling image classifiers. We propose a geometric
framework to generate adversarial examples in one of the
most challenging black-box settings where the adversary
can only generate a small number of queries, each of them
returning the top-1 label of the classifier. Our framework
is based on the observation that the decision boundary of
deep networks usually has a small mean curvature in the
vicinity of data samples. We propose an effective iterative
algorithm to generate query-efficient black-box perturbations with small ℓp norms for p ≥ 1, which is confirmed
via experimental evaluations on state-of-the-art natural image classifiers. Moreover, for p = 2, we theoretically show
that our algorithm actually converges to the minimal ℓ2-
perturbation when the curvature of the decision boundary
is bounded. We also obtain the optimal distribution of the
queries over the iterations of the algorithm. Finally, experimental results confirm that our principled black-box attack
algorithm performs better than state-of-the-art algorithms
as it generates smaller perturbations with a reduced number of queries. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/GeoDA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/GeoDA/GeoDA.zip">code下载</a> <br>  </p></div><br>









    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/Hard-label%20based%20Small%20Query%20Black-box%20Adversarial%20Attack.pdf">Hard-label based Small Query Black-box Adversarial Attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> We consider the hard-label based black-box adversarial attack setting which solely observes the target
model’s predicted class. Most of the attack methods in
this setting suffer from impractical number of queries
required to achieve a successful attack. One approach
to tackle this drawback is utilising the adversarial transferability between white-box surrogate models and blackbox target model. However, the majority of the methods adopting this approach are soft-label based to take
the full advantage of zeroth-order optimisation. Unlike mainstream methods, we propose a new practical
setting of hard-label based attack with an optimisation
process guided by a pre-trained surrogate model. Experiments show the proposed method significantly improves
the query efficiency of the hard-label based black-box attack across various target model architectures. We find
the proposed method achieves approximately 5 times
higher attack success rate compared to the benchmarks,
especially at the small query budgets as 100 and 250. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SQBA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SQBA/SQBA.zip">code下载</a> <br>  </p></div><br>







    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/Policy-Driven%20Attack%EF%BC%9A%20Learning%20to%20Query%20for%20Hard-label%20Black-box%20Adversarial%20Examples.pdf">Policy-Driven Attack： Learning to Query for Hard-label Black-box Adversarial Examples</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> To craft black-box adversarial examples, adversaries need to query the victim
model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision
(i.e., the hard-label prediction) of the victim model is available. In this paper, we
propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions
of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query
complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for
reproducing our results are available at https://github.com/ZiangYan/
pda.pytorch. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/policy_driven_attack/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/policy_driven_attack/policy_driven_attack.zip">code下载</a> <br>  </p></div><br>






    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/QEBA%EF%BC%9A%20Query-Efficient%20Boundary-Based%20Blackbox%20Attack.pdf">QEBA： Query-Efficient Boundary-Based Blackbox Attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Machine learning (ML), especially deep neural networks
(DNNs) have been widely used in various applications, including several safety-critical ones (e.g. autonomous driving). As a result, recent research about adversarial examples has raised great concerns. Such adversarial attacks
can be achieved by adding a small magnitude of perturbation to the input to mislead model prediction. While several whitebox attacks have demonstrated their effectiveness,
which assume that the attackers have full access to the machine learning models; blackbox attacks are more realistic in practice. In this paper, we propose a Query-Efficient
Boundary-based blackbox Attack (QEBA) based only on
model’s final prediction labels. We theoretically show why
previous boundary-based attack with gradient estimation
on the whole gradient space is not efficient in terms of query
numbers, and provide optimality analysis for our dimension
reduction-based gradient estimation. On the other hand, we
conducted extensive experiments on ImageNet and CelebA
datasets to evaluate QEBA. We show that compared with
the state-of-the-art blackbox attacks, QEBA is able to use a
smaller number of queries to achieve a lower magnitude
of perturbation with 100% attack success rate. We also
show case studies of attacks on real-world APIs including
MEGVII Face++ and Microsoft Azure. </p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/QEBA/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/QEBA/QEBA.zip">code下载</a> <br>  </p></div><br>







    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/Sign-OPT%EF%BC%9A%20A%20Query-Efficient%20Hard-label%20Adversarial%20Attack.pdf">Sign-OPT： A Query-Efficient Hard-label Adversarial Attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> We study the most practical problem setup for evaluating adversarial robustness
of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries
are allowed and only the decision is provided to a queried data input. Several
algorithms have been proposed for this problem but they typically require huge
amount (>20,000) of queries for attacking one example. Among them, one of
the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack
can be modeled as an optimization problem where the objective function can be
evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization
formulation but propose to directly estimate the sign of gradient at any direction
instead of the gradient itself, which enjoys the benefit of single query. Using
this single query oracle for retrieving sign of directional derivative, we develop
a novel query-efficient Sign-OPT approach for hard-label black-box attack. We
provide a convergence analysis of the new algorithm and conduct experiments
on several models on MNIST, CIFAR-10 and ImageNet. We find that Sign-OPT
attack consistently requires 5× to 10× fewer queries when compared to the current
state-of-the-art approaches, and usually converges to an adversarial example with
smaller perturbation.</p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SignOPT/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SignOPT/SignOPT.zip">code下载</a> <br>  </p></div><br>






    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/SurFree%EF%BC%9A%20a%20fast%20surrogate-free%20black-box%20attack.pdf">SurFree： a fast surrogate-free black-box attack</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Machine learning classifiers are critically prone to evasion attacks. Adversarial examples are slightly modified
inputs that are then misclassified, while remaining perceptively close to their originals. Last couple of years have
witnessed a striking decrease in the amount of queries a
black box attack submits to the target classifier, in order
to forge adversarials. This particularly concerns the black
box score-based setup, where the attacker has access to top
predicted probabilites: the amount of queries went from to
millions of to less than a thousand.
This paper presents SurFree, a geometrical approach
that achieves a drastic reduction in the amount of queries
in the hardest setup: black box decision-based attacks (only
the top-1 label is available). We first highlight that the
most recent attacks in that setup, HSJA [3], QEBA [14]
and GeoDA [23] all perform costly gradient surrogate estimations. SurFree proposes to bypass these, by instead
focusing on careful trials along diverse directions, guided
by precise indications of geometrical properties of the classifier decision boundaries. We motivate this geometric approach before performing a head-to-head comparison with
previous attacks with the amount of queries as a first class
citizen. We exhibit a faster distortion decay under low query
amounts (few hundreds to a thousand), while remaining
competitive at higher query budgets.</p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SurFree/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/SurFree/SurFree.zip">code下载</a> <br>  </p></div><br>





    <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://xuezhiqiang0621.github.io/hard_label_attack/paper/Triangle%20Attack%EF%BC%9A%20A%20Query-efficient.pdf">Triangle Attack： A Query-efficient</a> <br> <button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Decision-based attack poses a severe threat to real-world applications since it regards the target model as a black box and only accesses the hard prediction label. Great efforts have been made recently to
decrease the number of queries; however, existing decision-based attacks
still require thousands of queries in order to generate good quality adversarial examples. In this work, we find that a benign sample, the current
and the next adversarial examples can naturally construct a triangle in a
subspace for any iterative attacks. Based on the law of sines, we propose a
novel Triangle Attack (TA) to optimize the perturbation by utilizing the
geometric information that the longer side is always opposite the larger
angle in any triangle. However, directly applying such information on the
input image is ineffective because it cannot thoroughly explore the neighborhood of the input sample in the high dimensional space. To address
this issue, TA optimizes the perturbation in the low frequency space for
effective dimensionality reduction owing to the generality of such geometric property. Extensive evaluations on ImageNet dataset show that
TA achieves a much higher attack success rate within 1,000 queries and
needs a much less number of queries to achieve the same attack success
rate under various perturbation budgets than existing decision-based attacks. With such high efficiency, we further validate the applicability of
TA on real-world API, i.e., Tencent Cloud API.</p></div>
    <p style="margin:0"><button class="accordion">
      算法原理
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/triangle_attack/readme.md">算法原理</a> <br>  </p></div>
    <p style="margin:0"><button class="accordion">
      code
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <a href="https://xuezhiqiang0621.github.io/hard_label_attack/code/triangle_attack/triangle_attack.zip">code下载</a> <br>  </p></div><br>




















    <hr>

    <h2><a id="works-in-progress" class="anchor" href="#workinprogress" aria-hidden="true"><span class="octicon octicon-link"></span></a>Works in Progress</h2>

    <p style="margin:0"> <b>Beating the Heat: Temperature and Spatial Reallocation over the Long Run</b> <br> with <a href="https://www.christosmakridis.com/">Christos Makridis</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Does temperature affect real economic activity? Using the annual Current Population Survey between 1963 and 2015, we show that there is no association between temperature and earnings, hours, or output after controlling for time-invariant spatial heterogeneity and time-varying demographic factors. These results are robust to five separate sources of micro-data, different sampling horizons, functional forms, spatial measures of temperature, and subsets of the data. This paper studies the relationship between temperature and productivity across space and time. Motivated by these null results, we develop a spatial equilibrium model where temperature can affect not only firm productivity, but also individual locational choice. After calibrating the model, we use it to disentangle the role of reallocation versus actual productivity losses in the U.S. economy between 1980 and 2015. Nearly all of the variation is driven by reallocation. We subsequently use the model to evaluate a counterfactual climate scenario and recover a new spatial equilibrium for the U.S. economy by 2050. </p></div><br>

    <p style="margin:0"> <b>The Role of Supply and Demand Factors in Explaining the Migration of College Majors</b> <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>  This paper documents a new stylized fact about the United States labor market: internal migration rates are dramatically different across college majors. For some college majors, migration rates are even lower than those without a college degree. I relate major migration rates with majors' spatial concentration and find that a major's spatial concentration explains about one fourth of the cross-major variation in migration rates. With this descriptive evidence as a guide, I estimate a structural model of locational choice where college graduates have heterogeneous preferences---at the detailed major level---for living close to home, and for working in a location with a high concentration of their fellow majors. Using estimates of the structural model, I decompose the cross-major migration rates into supply and demand factors and find that supply factors (i.e. moving costs) explain the vast majority of differences in migration rates across majors. My findings underscore the difficulty in attracting college majors to a particular location using demand-side investments. My results also highlight the importance of place in determining the labor market outcomes of college majors.</p></div><br>

    <p style="margin:0"> <b>Measuring Women's Wage Returns to Human Capital</b> <br> with <a href="https://bschool.pepperdine.edu/about/people/faculty/jared-ashworth-professor-of-economics/">Jared Ashworth</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> This paper estimates the wage returns to schooling and actual early work experiences for young women using the NLSY surveys. We estimate a dynamic model of young women’s schooling, work, marriage and fertility decisions in order to account for the likely impacts on future wages. The results of the model without controlling for the decision process indicate a negative effect of children on earnings. Our full model explains how this occurs, demonstrating the importance of modeling these key life decisions, while controlling for unobserved ability.</p></div><br>

    <p style="margin:0"> <b>What is the Role of College Athletics? An Analysis of the Population of NCAA Athletes</b> <br> with Ahmed El Fatmaoui. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Coming soon.</p></div><br>

    <p style="margin:0"> <b>Credit Constraints, College Major Choices, and Upward Mobility</b> <br> with <a href="https://www.menakahampole.com/">Menaka Hampole</a> and <a href="https://sites.google.com/view/jconzelmann">Johnathan G. Conzelmann</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> We investigate how easing credit constraints affects college graduates' choices in majors and migration, focusing on the impact of student loan debt. Using a rich dataset combining university, credit bureau, and other records, we examine the effects of loan reduction or "No Loan" policies at select universities on students' major choices and migration decisions. By analyzing data across different dimensions, including parental financial constraints and socioeconomic networks, we aim to provide new insights into the relationship between education financing and upward economic mobility. </p></div><br>

    <p style="margin:0"> <b>Elevation in Obesity: Uncovering the Geographical Aspects of Health</b> <br> with <a href="https://sites.google.com/site/pallabghoshou/">Pallab Ghosh</a>. <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> Coming soon.</p></div><br>

    <p style="margin:0"> <b>Why Are Americans Still Becoming More Obese? Causes Beyond Caloric Imbalances</b> <br><button class="accordion">
      Abstract
    </button>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> In the U.S. and other developed countries, obesity rates have risen despite stabilized caloric intake and increased physical activity, challenging the traditional energy balance model. This proposal introduces a metabolic-centric model, grounded in recent biochemistry advances, to explain this paradox. It posits that metabolic disruptions, driven by factors such as increased levels of Omega-6 fatty acids in the food supply, environmental contaminants, or microbiome changes, are key. I critically review data sources and empirical evidence supporting these channels. Furthermore, I outline how economists, with their unique perspectives on incentives and tradeoffs, can contribute to innovative solutions for the obesity epidemic. </p></div><br>

      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
